---
title: Talking about the problems and limitations of Kubernetes
---


# Talking about the problems and limitations of Kubernetes


Kubernetes, released in 2014, has become the de-facto standard in container orchestration today, and I'm sure developers who talk about Kubernetes will recount the phenomenon over and over again. As the chart below shows, most individuals or teams today choose Kubernetes to manage containers, and 75% of people use Kubernetes in production environments.

In the context of this universal learning and use of Kubernetes, it should also be very clear what the limitations of Kubernetes are. Although Kubernetes can solve most of the problems in the container orchestration space, there are still some scenarios that it is difficult or even impossible to handle, and only with a clear understanding of these potential risks can we better navigate this technology.

## Cluster Management
A cluster is a group of computers that can work together. We can consider all the computers in a cluster as a whole, and all the resource scheduling systems are managed with the cluster as a dimension. Here we briefly discuss several complex issues facing Kubernetes cluster management.

### Horizontal Scalability
Cluster size is one of the most important metrics we need to look at when evaluating resource management systems, yet Kubernetes can manage far smaller clusters than other resource management systems in the industry. Why cluster size is important? Let's look at another equally important metric - resource utilization. Many engineers may not have requested resources on a public cloud platform, and these resources are quite expensive. RMB 2.

Most clusters use 48 CPU or 64 CPU physical machines or VMs as nodes in the cluster, so if we need to have 5,000 nodes in our cluster, then these nodes would cost about $8,000,000 per month or about 50,000,000 RMB. 500,000 per month.

Most online tasks have low resource utilization, and a larger cluster means that more workloads can be run, and multiple peaks and valleys of different loads can be deployed together to achieve overselling, which can significantly improve the resource utilization of the cluster. The Kubernetes community advertises this to the public.

The Kubernetes community advertises to the public that a single cluster supports up to 5,000 nodes, no more than 150,000 total Pods, no more than 300,000 total containers, and no more than 100 single-node Pods.3 Compared to Apache Mesos clusters with tens of thousands of nodes and Microsoft YARN clusters with 50,000 nodes,4 The cluster size of Kubernetes is an order of magnitude worse. While Aliyun engineers have also achieved 5-digit cluster sizes by optimizing various components of Kubernetes, there is a relatively large gap compared to other resource management approaches5.

It is important to note that while the Kubernetes community claims to support 5,000 nodes in a single cluster and has a variety of integration tests to ensure that every change does not affect its scalability6, Kubernetes is complex and there is no way to guarantee that every feature you use will not have problems during the scaling process. And in a production environment, we may even run into bottlenecks when the cluster scales to 1000 ~ 1500 nodes.

Every large company of any size wants to implement a larger Kubernetes cluster, but this is not a simple problem that can be solved by changing a few lines of code, it may require us to limit the use of some features in Kubernetes, and problems may occur with etcd, API servers, schedulers, and controllers during the scaling process. Some developers in the community have noticed some of these issues, such as adding caching to nodes to reduce the load on API servers7, but it is still difficult to push for similar changes, and interested parties can try to push for similar projects in the community.

### Multi-cluster management
Even if a Kubernetes cluster could one day reach 50,000 nodes, we would still need to manage multiple clusters, and multicluster management is something the Kubernetes community is currently exploring. In the author's opinion, multi-clustering in Kubernetes poses three major problems: resource imbalance, difficulty in accessing across clusters, and increased operational and management costs.

#### kubefed
The first is tube fed, a solution from the Kubernetes community that provides both cross-cluster resource and network management, developed by the community's SIG Multi-Cluster interest group.

tube-fed manages metadata in a multicluster through a centralized federation console. The upper-level console creates corresponding federation objects for the resources in the manager cluster, e.g. FederatedDeployment.
```
kind: FederatedDeployment
...
spec:
  ...
  overrides:
  # Apply overrides to cluster1
    - clusterName: cluster1
      clusterOverrides:
        # Set the replicas field to 5
        - path: "/spec/replicas"
          value: 5
        # Set the image of the first container
        - path: "/spec/template/spec/containers/0/image"
          value: "nginx:1.17.0-alpine"
        # Ensure the annotation "foo: bar" exists
        - path: "/metadata/annotations"
          op: "add"
          value:
            foo: bar
        # Ensure an annotation with key "foo" does not exist
        - path: "/metadata/annotations/foo"
          op: "remove"
        # Adds an argument `-q` at index 0 of the args list
        # this will obviously shift the existing arguments, if any
        - path: "/spec/template/spec/containers/0/args/0"
          op: "add"
          value: "-q"
```
The upper control panel generates the corresponding Deployment based on the FederatedDeployment specification file and pushes it to the lower cluster, which can normally create a specific number of replicas based on the definition in the Deployment.

FederatedDeployment is only the simplest distribution strategy. In production environments where we want to implement complex features such as disaster recovery through federated clusters, we can use ReplicaSchedulingPreference to implement a more intelligent distribution strategy across clusters.

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: test-deployment
  namespace: test-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 9
  clusters:
    A:
      minReplicas: 4
      maxReplicas: 6
      weight: 1
    B:
      minReplicas: 4
      maxReplicas: 8
      weight: 2
```
The above scheduling strategy allows workloads to be weighted across clusters, and instances to be migrated to other clusters when they are under-resourced or even have problems, which improves the flexibility and availability of service deployment and allows infrastructure engineers to better balance the load across multiple clusters.

We can consider the main role of tube feeding to be to form multiple loosely-coupled federated clusters and to provide more advanced networking and deployment capabilities so that we can more easily address some of the resource imbalance and connectivity issues between clusters, however, the focus of this project does not include cluster lifecycle management.

## Cluster Interface
The Cluster API is also a multicluster management-related project in the Kubernetes community, developed by the SIG Cluster-Lifecycle group, whose main goal is to simplify the preparation, update, and maintenance of multi clusters through a declarative API, whose scope of responsibility you can find in the project's design proposal9. You can find its terms of reference in the project's design proposal9.

The most important resource in this project is the Machine, which represents the nodes in a Kubernetes cluster. When this resource is created, the provider-specific controller initializes and registers the new node into the cluster based on the definition of the machine, and also performs operations to reach the user's state when the resource is updated or deleted.

This strategy is somewhat similar to Ali's approach to multi-cluster management in that they both use declarative APIs to define the state of machines and clusters, and then use Kubernetes' native Operator model to manage lower-level clusters in higher-level clusters, which can greatly reduce the cost of cluster operations and improve the operational efficiency of clusters10, although similar projects do not consider cross-cluster However, similar projects do not consider cross-cluster resource management and network management.

### Application Scenarios
In this section, we talk about some interesting application scenarios in Kubernetes, including the current state of application distribution, batch scheduling tasks, and hard multi-tenancy support in clusters, which are of interest to the community and are currently a blind spot in Kubernetes.

### Application Distribution
The main Kubernetes project provides several minimal ways to deploy applications, namely Deployment, StatefulSet, and DaemonSet. These resources are for stateless services, stateful services, and daemons on nodes, respectively, and they provide the most basic policies, but they cannot handle more complex applications.

With the introduction of CRD, the community's application management group (SIG Apps) now largely refrains from introducing large changes to the Kubernetes master repository. Most changes are patches on existing resources, and many common scenarios, such as the once-only DaemonSet11 and features such as Canary and Bluegreen deployments, have many issues with the resources now. For example, StatefulSet is stuck in the initialization container and cannot be rolled back and updated12.
Understandably, the community does not want to maintain more basic resources in Kubernetes, as a few basic resources can cover 90% of the scenarios, leaving the rest of the complex scenarios for other communities to implement by way of CRD. However, the authors believe that if the community can implement more high-quality components upstream, this is a valuable and important work for the whole ecosystem, and it should be noted that if you want to be a contributor in the Kubernetes project, SIG Apps may not be a good choice.

### Batch Scheduling
Running workloads such as machine learning, batch tasks, and streaming tasks have not been Kubernetes' forte since day one, and most companies use Kubernetes to run online services to handle user requests and Yarn-managed clusters to run batch loads.

Most online tasks are stateless services that can be migrated across machines and have very strong dependencies on each other; however, many offline tasks have complex topologies, with some tasks requiring multiple jobs to execute together and some tasks requiring sequential execution according to dependencies.

Before the Kubernetes scheduler introduced the scheduling framework, all Pods were unrelated to the scheduler, but with the scheduling framework, we can implement more complex scheduling policies in the scheduling system, such as PodGroup13 that guarantees simultaneous scheduling of a group of Pods, which is useful for Spark and TensorFlow tasks.
```
# PodGroup CRD spec
apiVersion: scheduling.sigs.k8s.io/v1alpha1
kind: PodGroup
metadata:
  name: nginx
spec:
  scheduleTimeoutSeconds: 10
  minMember: 3
minMember: 3 ---
# Add a label `pod-group.scheduling.sigs.k8s.io` to mark the pod belongs to a group
labels:
  pod-group.scheduling.sigs.k8s.io: nginx
```

A volcano is also a batch task management system built on Kubernetes14 that can handle machine learning, deep learning, and other big data applications, supporting multiple frameworks including TensorFlow, Spark, PyTorch, and MPI.


While Kubernetes can run some batch tasks, it is still very far from replacing older resource management systems such as Yarn in this space, and it is believed that most companies will maintain both Kubernetes and Yarn technology stacks to manage and run different types of workloads for a longer period.

Hard Multi-Tenancy
Multi-tenancy means that the same software instance can serve different groups of users. Multi-tenancy in Kubernetes means that multiple users or groups of users use the same Kubernetes cluster, and today it is difficult to achieve hard multi-tenancy support, which means that multiple tenants in the same cluster do not affect each other and are not aware of each other's presence.

Hard multi-tenancy is a very important and difficult topic in Kubernetes. A typical multi-tenant scenario is a shared apartment, where multiple tenants share the infrastructure of the house, and hard multi-tenancy requires that multiple visitors do not affect each other.

Although Kubernetes uses namespaces to divide groups of VMs, it is difficult to achieve true multi-tenancy. Here is a brief list of the benefits of multi-tenancy support.

The additional deployment costs associated with Kubernetes are prohibitive for small clusters; stable Kubernetes clusters typically require at least three master nodes running etcd, and if most clusters are small, these additional machines can introduce a high level of additional overhead.
containers running in Kubernetes may need to share physical and virtual machines, and some developers may have experienced their services being impacted by other operations within their companies because the containers on the host may isolate CPU and memory resources, but not resources such as I/O, network and CPU cache, which is relatively difficult to isolate.
If Kubernetes can achieve hard multi-tenancy, it will not only be a boon for cloud providers and users of small clusters, but it can also isolate the impact between different containers and prevent potential security issues, but this is still difficult to achieve at this stage.

## Conclusion
Every technology has its lifecycle, the lower the technology lifecycle will be longer, and the higher the technology lifecycle will be shorter, although Kubernetes is the leader of a today's container world, no one can say the future of things. We should always be aware of the strengths and weaknesses of the tools at hand, spend some time learning the essence of the design in Kubernetes, but if at some point in the future Kubernetes becomes a thing of the past, we should be happy that a better tool will replace it.

## Reference

1. Kubernetes and Container Security and Adoption Trends https://www.stackrox.com/kubernetes-adoption-security-and-market-share-for- containers/︎

2. AWS Pricing Calculator https://calculator.aws/#/createCalculator/EC2 ︎

3. Considerations for large clusters https://kubernetes.io/docs/setup/best-practices/cluster-large/ ︎

4. How Microsoft drives exabyte analytics on the world's largest YARN cluster https://azure.microsoft.com/en-us/blog/how-microsoft- drives-exabyte-analytics-on-the-world-s-largest-yarn-cluster/ ︎

5. How to design a 10,000 scale K8s cluster management system for Anthem? https://www.sofastack.tech/blog/ant-financial-managing-large-scale-kubernetes-clusters/ ︎

6. sig-scalability-kubemark dashboard https://testgrid.k8s.io/sig-scalability-kubemark#kubemark-5000 ︎

7. Node-local API cache #84248 https://github.com/kubernetes/kubernetes/issues/84248 ︎

8. Multicluster Special Interest Group https://github.com/kubernetes/community/tree/master/sig-multicluster ︎

9. Cluster API Scope and Objectives https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/scope-and-objectives.md ︎

10. Demystifying Kubernetes as a service - How Alibaba cloud manages 10,000s of Kubernetes clusters https://www.cncf.io/blog/2019/12/12/ demystifying-Kubernetes-as-a-service-how-does-Alibaba-cloud-manage-10,000s-of-Kubernetes-clusters/︎

11. Run job on each node once to help with setup #64623 https://github.com/kubernetes/kubernetes/issues/64623 ︎

12. StatefulSet does not upgrade to a newer version of manifests #78007 https://github.com/kubernetes/kubernetes/issues/78007 ︎

13. So scheduling based on PodGroup CRD https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/42-podgroup-coscheduling ︎

14. Volcano - A Kubernetes Native Batch System https://github.com/volcano-sh/volcano ︎

15. Kubernetes Working Group for Multi-Tenancy https://github.com/kubernetes-sigs/multi-tenancy ︎