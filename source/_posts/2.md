---
title: Why Linux and macOS don't need defragmentation？
---

# Why Linux and macOS don't need defragmentation

I believe that many software engineers today are using Linux or macOS systems, unlike Windows, we hardly see the concept of disk defragmentation, and from personal experience, the author has not defragmented a disk in macOS for the past seven or eight years, and you won't find any related operations in today's disk tools, only through the disk until command, You can only set whether defragmentation is turned on or off for a particular disk with the command diskutil.

In a previous article, we analyzed why early Windows operating systems might need to defragment their disks every once in a while.1 There are two reasons behind this problem: one is that Windows uses a very simple file system called FAT, which is designed in such a way that the same files may be scattered in different locations on the disk, and the other is that solid-state drives were not popular in the ancient times. mechanical hard drives had poor random read and write performance.

Linux and macOS systems do not need defragmentation for the opposite reason that Windows does.

Linux and macOS use file systems that either reduce the probability of fragmentation or implement automatic defragmentation features.
solid-state drives have different characteristics than mechanical drives, and defragmentation may not only not significantly help improve the read and write performance, but also be detrimental to the life of the hardware.
## File systems
Linux generally uses Ext2, Ext3, and Ext4 file systems, with most Linux distributions today opting for Ext4. Unlike Windows, which stores multiple files consecutively, Linux scatters files to different parts of the disk, while leaving some space between files to ensure that they are not fragmented when modified or updated.

Most of today's macOS uses the APFS file system3, which is a file system optimized by Apple specifically for devices such as solid-state drives. Earlier HFS and HFS+ used a block-based (Extent) design, where each block contains a serial number and a contiguous section of storage space, and this allocation looks for several contiguous blocks in the file system to provide the space needed.

Both Linux and macOS file systems are designed to be block-based, and the disk space allocation is relatively reasonable, so there are no fragmented disks on Windows systems.

In addition to the fact that the file system is designed to avoid fragmentation, both Linux and macOS also introduce a delayed space allocation strategy, which delays disk writes as much as possible through buffers, which not only reduces the probability of disk swipes, but also increases the probability of files being written to adjacent blocks, but this mechanism is not without side effects, as more data may be lost in the event of a power failure or crash. However, this mechanism is not without side effects, as more data may be lost in case of a system power failure or crash.

If fragmentation does occur on the disk, the Linux and macOS file systems will attempt to move the fragmented files without the need for additional defragmentation tools, which is a much better user experience than manually triggering time-consuming defragmentation. macOS HFS+ also supports real-time defragmentation, which is triggered when the following conditions are met4.

The file is less than 20 MB.
the file is stored on more than 8 blocks.
the file has not been updated at the last minute.
the system has been booted for three minutes.
In most cases, the percentage of disk fragmentation in these operating systems is very low and fragmentation only starts when the disk is running low on space, so at that point, we need a larger disk or an updated computer, not defragmentation of the disk.

## Solid State Drives
Solid-state drives have been a storage medium for three decades, but because they have been so expensive in the past, they have not been popular in data centers and personal computers. Even today, the price of mechanical disks has a significant advantage over SSDs.

New storage media brings new features and performance, as we described in a previous article, because of the mechanical structure of a mechanical hard drive, its random I/O, and sequential I/O performance can vary by hundreds of times, defragmentation can combine data scattered across the disk into one place, reducing the number of random I/Os will naturally improve the performance of reading and writing files.

Although there is a difference in performance between sequential I/O and random I/O on SSDs, the difference may be between a dozen and tens of times, and the random I/O latency of SSDs is tens or even thousands of times better than that of mechanical disks, so the benefits of defragmentation on SSDs are now limited.

Although SSDs as electronic components have better performance, SSDs are limited to the number of cycles of erasure, also known as P/E. Their lifespan is limited compared to mechanical drives. If a 512 GB solid-state drive is erased 1000 times, each time the data is written will consume life, and when the number of erasures reaches 1000 the drive will be scrapped, defragmentation is actively moving the data on the drive, which naturally affects the life of the hardware.

## Summary
In software engineering there is a very interesting phenomenon, do hardware and infrastructure engineers are desperately trying to optimize the performance of the system, but the application layer of engineers often do not care about the small differences in performance, and this is also the result of differences in job responsibilities, different positions determine the different concerns.

The evolution and innovation of hardware deeply affect the design of upper-level software, and it is extremely difficult to design a universal system that does not take into account the characteristics of the underlying hardware when designing a file system, so it is impossible to take full advantage of the performance provided by the hardware and get the desired results. Here is a summary of two reasons why Linux and macOS do not require defragmentation.

(a) The block allocation-based design of the file system makes the probability of fragmentation on the disk very low, and the delayed allocation and automatic defragmentation policy free the operating system user from the need to consider disk fragmentation in most cases.
the random read/write performance of SSDs is far better than that of mechanical drives, and while there are performance differences between random and sequential read/write, they are not as great as those of mechanical drives, while frequent defragmentation can also affect the life of an SSD.
At the end of the day, let's look at some more open related questions, and interested readers can think carefully about the following questions, which, unlike the previous ones, will be answered by the authors in the following article.

Why is there an upper limit on the number of erases for solid-state drives?
Under what conditions are mechanical hard drives more likely to be damaged?
If you have questions about the content of this article or want to learn more about the reasons behind some of the design decisions in software engineering, you can leave a comment below the blog and the author will promptly respond to any questions related to this article and select the appropriate topic for a follow-up.

## Recommended Reading
《Why CPU access to hard drives is slow》
《Why HugePages can improve database performance》
## References
Why early Windows needed defragmentation https://draveness.me/whys-the-design-windows-defragmentation/︎

Is there a tool to visualize a filesystem allocation map on Linux? https://unix.stackexchange.com/questions/30743/is-there-a-tool-to-visualize -a-filesystem-allocation-map-on-Linux ︎

Wikipedia: Apple File System https://en.wikipedia.org/wiki/Apple_File_System ︎

HFS+ and File System Fragmentation https://developercoach.com/file-system-fragmentation/ ︎

